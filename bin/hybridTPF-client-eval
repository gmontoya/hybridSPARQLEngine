#!/usr/bin/env node
/*! @license ©2013 Ruben Verborgh - Multimedia Lab / iMinds / Ghent University */
/* Command-line utility to execute SPARQL queries over triple pattern fragments. */

var ldf = require('../hybridTPF-client');

// Retrieve and check arguments
var args = require('minimist')(process.argv.slice(2));
if (args.listformats)
  return Object.keys(ldf.SparqlResultWriter.writers).forEach(function (t) { console.log(t); });
if (!(args.q || args.f) && args._.length < 1 || args._.length > 2 || args.h || args.help) {
  console.error('usage: hybridTPF-client-eval [startFragment] query.sparql [-c config.json] ' +
                                          '[-t application/json] [-l logLevel] [--help]' +
                                          '[--threshold w] [--endpointUrl z]' + // Added for the hybrid SPARQL engine
                                          '[--maxNumberOfMappings x]' + '[--outputFileNumber y]' );
  console.error('       hybridTPF-client-eval --listformats (Lists supported result formats for -t)');
  return process.exit(1);
}

// Load main libraries (postponed as to here for speed)
var fs = require('fs'),
    path = require('path'),
    N3 = require('n3'),
    Logger = ldf.Logger;

// Parse and initialize configuration
var configFile = args.c ? args.c : path.join(__dirname, '../config-default.json'),
    config = JSON.parse(fs.readFileSync(configFile, { encoding: 'utf8' })),
    queryFile = args.f || args.q || args._.pop(),
    startFragment = args._.pop() || config.startFragment,
    query = args.q || (args.f || fs.existsSync(queryFile) ? fs.readFileSync(queryFile, 'utf8') : queryFile),
    mimeType = args.t || 'application/json';

// Added for the hybrid SPARQL engine
var defaultThreshold = 100;
config.threshold = args.threshold ? args.threshold : (config.threshold ? config.threshold : defaultThreshold);
config.endpointUrl = args.endpointUrl ? args.endpointUrl : (config.endpointUrl ? config.endpointUrl : 'http://172.19.2.100:8891/sparql?default-graph-uri=http%3A%2F%2Fdbpedia&query=');

config.maxNumberOfMappings = args.maxNumberOfMappings ? args.maxNumberOfMappings : (config.maxNumberOfMappings ? config.maxNumberOfMappings : 10)

// Configure logging
var measurementsFile  = 'eval_hybridTPF_' + args.outputFileNumber + '.csv';
var measurementsFile2 = 'eval2_hybridTPF_' + args.outputFileNumber + '.csv';
Logger.setLevel(args.l || 'warning');

function messageForMeasurementsFile2( chunkSizeCounters ) {
  var msg = ""
  for ( var i = 0; i <= config.maxNumberOfMappings; i++ ) {
    if ( chunkSizeCounters[i] === undefined || ! chunkSizeCounters[i] ) {
      chunkSizeCounters[i] = 0;
    }
  }
  return chunkSizeCounters.join(',');
}

var start = new Date();
var DEBUGfirstTime, DEBUGtime, DEBUGfirstHttp, DEBUGhttp, DEBUGdata, DEBUGtotal = 0, seDEBUGcalls, seDEBUGdata;

var timeoutInMins = args.timeoutInMins ? args.timeoutInMins : (config.timeoutInMins ? config.timeoutInMins : 5);

setTimeout( function () {
  DEBUGtime = new Date() - start;
  DEBUGhttp = config.fragmentsClient._httpClient.DEBUGcalls;
  DEBUGtps = config.fragmentsClient._numberOfTriplePatternsInQuery;
  DEBUGdata = config.fragmentsClient._overallNumberOfTriplesReceived;
  seDEBUGcalls = config.fragmentsClient._numberOfSPARQLEndpointCalls;
  seDEBUGdata = config.fragmentsClient._numberOfTTFromSPARQLEndpoint;
  //fs.appendFileSync(measurementsFile, 'seDEBUGcalls: '+seDEBUGcalls);
  //fs.appendFileSync(measurementsFile, 'seDEBUGdata: '+seDEBUGdata);
  fs.appendFileSync(measurementsFile, [queryFile,DEBUGtps,DEBUGfirstTime, DEBUGfirstHttp, DEBUGtime, DEBUGhttp, DEBUGdata, DEBUGtotal, seDEBUGcalls, seDEBUGdata, 'TIMEOUT', timeoutInMins].join(',') + '\n');
  fs.appendFileSync(measurementsFile2, queryFile + ',' + messageForMeasurementsFile2(config.fragmentsClient._chunkSizeCounters) + ',TIMEOUT,' + timeoutInMins + '\n');
  process.exit();
}, timeoutInMins*60*1000 );

// Execute the query and output its results
config.fragmentsClient = new ldf.FragmentsClient(startFragment, config);
try {
  var sparqlIterator = new ldf.SparqlIterator(query, config), writer;
  switch (sparqlIterator.queryType) {
  // Write JSON representations of the rows or boolean
  case 'ASK':
  case 'SELECT':
    writer = new ldf.SparqlResultWriter(mimeType, sparqlIterator);
    writer.on('data', function (data) {
      if (data.length > 3)
        DEBUGtotal++;
      if (!DEBUGfirstTime && data.length > 3) {
        DEBUGfirstTime = new Date() - start;
        DEBUGfirstHttp = config.fragmentsClient._httpClient.DEBUGcalls;
      }
      process.stdout.write(data);
    });
    break;
  // Write an RDF representation of all results
  case 'CONSTRUCT':
  case 'DESCRIBE':
    writer = new N3.Writer(process.stdout, config);
    sparqlIterator.on('data', function (triple) { writer.addTriple(triple); })
                  .on('end',  function () { writer.end(); });
    break;
  default:
    throw new ldf.SparqlIterator.UnsupportedQueryError(query);
  }

  // Report an error's stack trace
  sparqlIterator.on('error', function (error) {
    console.error('ERROR: An error occured during query execution.\n');
    console.error(error.stack);
    DEBUGtime = new Date() - start;
    DEBUGhttp = config.fragmentsClient._httpClient.DEBUGcalls;
    DEBUGtps = config.fragmentsClient._numberOfTriplePatternsInQuery;
    DEBUGdata = config.fragmentsClient._overallNumberOfTriplesReceived;
    seDEBUGcalls = config.fragmentsClient._numberOfSPARQLEndpointCalls;
    seDEBUGdata = config.fragmentsClient._numberOfTTFromSPARQLEndpoint;
    //fs.appendFileSync(measurementsFile, 'seDEBUGcalls: '+seDEBUGcalls);
    //fs.appendFileSync(measurementsFile, 'seDEBUGdata: '+seDEBUGdata);
    fs.appendFileSync(measurementsFile, [queryFile,DEBUGtps,DEBUGfirstTime, DEBUGfirstHttp, DEBUGtime, DEBUGhttp, DEBUGdata, DEBUGtotal, seDEBUGcalls, seDEBUGdata, 'ERROR', error].join(',') + '\n');
    fs.appendFileSync(measurementsFile2, queryFile + ',' + messageForMeasurementsFile2(config.fragmentsClient._chunkSizeCounters) + '\n');
    process.exit();
  });

  writer.on('end', function () {
    DEBUGtime = new Date() - start;
    DEBUGhttp = config.fragmentsClient._httpClient.DEBUGcalls;
    DEBUGtps = config.fragmentsClient._numberOfTriplePatternsInQuery;
    DEBUGdata = config.fragmentsClient._overallNumberOfTriplesReceived;
    seDEBUGcalls = config.fragmentsClient._numberOfSPARQLEndpointCalls;
    seDEBUGdata = config.fragmentsClient._numberOfTTFromSPARQLEndpoint;
    //console.log('config.fragmentsClient._numberOfTTFromSPARQLEndpoint: '+config.fragmentsClient._numberOfTTFromSPARQLEndpoint); console.log('config.fragmentsClient._numberOfSPARQLEndpointCalls: '+config.fragmentsClient._numberOfSPARQLEndpointCalls);
    //fs.appendFileSync(measurementsFile, 'seDEBUGcalls: '+seDEBUGcalls);
    //fs.appendFileSync(measurementsFile, 'seDEBUGdata: '+seDEBUGdata);
    fs.appendFileSync(measurementsFile, [queryFile,DEBUGtps,DEBUGfirstTime, DEBUGfirstHttp, DEBUGtime, DEBUGhttp, DEBUGdata, DEBUGtotal, seDEBUGcalls, seDEBUGdata].join(',') + '\n');
    fs.appendFileSync(measurementsFile2, queryFile + ',' + messageForMeasurementsFile2(config.fragmentsClient._chunkSizeCounters) + '\n');
    process.exit(0); //process.stdout.uncork(); //process.stdout.flush();
  });
}
// Report a synchronous error
catch (error) {
  console.error('ERROR: Query execution could not start.\n');
  switch (error.name) {
  case 'InvalidQueryError':
  case 'UnsupportedQueryError':
    console.error(error.message);
    break;
  default:
    console.error(error.stack);
  }
}
